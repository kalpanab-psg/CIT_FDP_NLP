{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "nlp_cit1.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "LBQ-iIaMg0mY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a9600d4e-d09d-40ab-afc7-1ab3f589f83e"
      },
      "source": [
        "import os\n",
        "import nltk\n",
        "import nltk.corpus\n",
        "nltk.download('punkt')\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QIcWCw292dj6"
      },
      "source": [
        "#TOKENIZATION\n",
        "\n",
        "Tokenization breaks a text paragraph to sentence / words\n",
        "\n",
        "We can understand the importance of each word in the sentence\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5lK3md9Qg421"
      },
      "source": [
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "\n",
        "speech_text='''\n",
        "  I have met, so far, 11 million youth like you in a decade?s time, in India and abroad. I have seen their hopes, \n",
        "  experienced their pains, walked with their aspirations and heard through their despair. All this experience made me \n",
        "  learn something about them, which I would like to share with you:I learnt, every youth wants to be unique, that is, YOU! But the world all around you, is doing its best, day and night, to make you just \"everybody else\".\n",
        "Being like everybody else is convenient at the first glance, but not satisfying in the long vision.\n",
        "The challenge, therefore, my young friends, is that you have to fight the hardest battle, which any human being\n",
        " can ever imagine to fight; and never stop fighting until you arrive at your destined place, that is, a UNIQUE YOU!\n",
        "Being unique will require excellence, let us understand what is excellence in more detail.\n",
        "Excellence is a self-imposed self-directed life-long process\n",
        "Excellence is not by accident. It is a process, where an individual, organization or nation, continuously strives \n",
        "to better oneself. The performance standards are set by themselves, they work on their dreams with focus and are\n",
        " prepared to take calculated risks and do not get deterred by failures as they move towards their dreams. \n",
        "Then they step up their dreams as they tend to reach the original targets.\n",
        " They strive to work to their potential, in the process, they increase their performance thereby multiplying further their\n",
        " potential and this is an unending life cycle phenomenon. They are not in competition with anyone else, but themselves.\n",
        " That is the culture of excellence.  \n",
        "\n",
        "'''\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bzhpTUAr5Qz3",
        "outputId": "6f613ff8-f177-479e-aeca-080a046760db"
      },
      "source": [
        "type(speech_text)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "str"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fr_I4sfu5Ouh"
      },
      "source": [
        "sentences=nltk.sent_tokenize(speech_text)\n",
        "words=nltk.word_tokenize(speech_text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hSVaJkZ5I3Bc",
        "outputId": "1ee8f533-f279-4d31-db58-690de4083e28"
      },
      "source": [
        "print(sentences)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['\\n  I have met, so far, 11 million youth like you in a decade?s time, in India and abroad.', 'I have seen their hopes, \\n  experienced their pains, walked with their aspirations and heard through their despair.', 'All this experience made me \\n  learn something about them, which I would like to share with you:I learnt, every youth wants to be unique, that is, YOU!', 'But the world all around you, is doing its best, day and night, to make you just \"everybody else\".', 'Being like everybody else is convenient at the first glance, but not satisfying in the long vision.', 'The challenge, therefore, my young friends, is that you have to fight the hardest battle, which any human being\\n can ever imagine to fight; and never stop fighting until you arrive at your destined place, that is, a UNIQUE YOU!', 'Being unique will require excellence, let us understand what is excellence in more detail.', 'Excellence is a self-imposed self-directed life-long process\\nExcellence is not by accident.', 'It is a process, where an individual, organization or nation, continuously strives \\nto better oneself.', 'The performance standards are set by themselves, they work on their dreams with focus and are\\n prepared to take calculated risks and do not get deterred by failures as they move towards their dreams.', 'Then they step up their dreams as they tend to reach the original targets.', 'They strive to work to their potential, in the process, they increase their performance thereby multiplying further their\\n potential and this is an unending life cycle phenomenon.', 'They are not in competition with anyone else, but themselves.', 'That is the culture of excellence.']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rnWwRVW8IWmP",
        "outputId": "f1409c8c-7819-49e8-9eab-b0acac913c3f"
      },
      "source": [
        "print(len(sentences))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "14\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ghm-tXCtIz6C",
        "outputId": "da549672-177c-497a-e14d-32900cb21944"
      },
      "source": [
        "print(len(words))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "318\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "loPxXXOSbLX_"
      },
      "source": [
        "#stopwords , stemming, lemmatization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y_Q4xBZkbKAh",
        "outputId": "62493e02-6ddb-4d94-8751-dfb61d6ea4e0"
      },
      "source": [
        "from nltk.stem import PorterStemmer\n",
        "e_words= [\"eat\", \"eaten\", \"eating\"]\n",
        "ps =PorterStemmer()\n",
        "for w in e_words:\n",
        "    rootWord=ps.stem(w)\n",
        "    print(rootWord)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "eat\n",
            "eaten\n",
            "eat\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4a0x1qisb8Y-",
        "outputId": "a19c788b-c024-45a3-e5f1-6c9b9cabd622"
      },
      "source": [
        "#Lemmatization\n",
        "nltk.download('wordnet')\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lpDAkW7Gcd4E",
        "outputId": "5be34dbd-a156-4822-a967-272c41974f70"
      },
      "source": [
        "from nltk.stem import \tWordNetLemmatizer\n",
        "wordnet_lemmatizer = WordNetLemmatizer()\n",
        "text = \"eat eaten eating\"\n",
        "tokenization = nltk.word_tokenize(text)\n",
        "for w in tokenization:\n",
        "\t\tprint(\"Lemma for {} is {}\".format(w, wordnet_lemmatizer.lemmatize(w)))  "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Lemma for eat is eat\n",
            "Lemma for eaten is eaten\n",
            "Lemma for eating is eating\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HizjPrr3c7LD"
      },
      "source": [
        "Steming is easy to implement\n",
        "If you look stemming for studies and studying, output is same (studi) but lemmatizer provides different lemma for both tokens study for studies and studying for studying. So when we need to make feature set to train machine, it would be great if lemmatization is preferred."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f4mB8Y9QQTtA"
      },
      "source": [
        "##POS tagging"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AXUJ0y-2b7QU"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JqxPkg6-6HUf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7c3b4cf8-e483-46d6-809e-ec15ee2fd47b"
      },
      "source": [
        "import nltk \n",
        "nltk.download('stopwords')\n",
        "\n",
        "from nltk.corpus import stopwords \n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize \n",
        "stop_words = set(stopwords.words('english')) \n",
        "  \n",
        "Martin_Luther_txt = \"\"\"\n",
        "I am happy to join with you today in what will go down in history as the greatest demonstration for freedom in the history of our nation. \n",
        "Five score years ago, a great American, in whose symbolic shadow we stand today, signed the Emancipation Proclamation. This momentous decree came as\n",
        " a great beacon light of hope to millions of Negro slaves  . who had been seared in the flames of withering injustice. It came as a joyous daybreak\n",
        "  to end the long night of their captivity. \"\"\"\n",
        "    \n",
        "# sent_tokenize is one of instances of  \n",
        "# PunktSentenceTokenizer from the nltk.tokenize.punkt module \n",
        "  \n",
        "tokenized = sent_tokenize(Martin_Luther_txt) \n",
        "for i in tokenized: \n",
        "      \n",
        "    # Word tokenizers is used to find the words  \n",
        "    # and punctuation in a string \n",
        "    wordsList = nltk.word_tokenize(i) \n",
        "  \n",
        "    # removing stop words from wordList \n",
        "    wordsList = [w for w in wordsList if not w in stop_words]  \n",
        "  \n",
        "    #  Using a Tagger. Which is part-of-speech  \n",
        "    # tagger or POS-tagger.  \n",
        "    tagged = nltk.pos_tag(wordsList) \n",
        "  \n",
        "    print(tagged) "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[('I', 'PRP'), ('happy', 'JJ'), ('join', 'NN'), ('today', 'NN'), ('go', 'VBP'), ('history', 'NN'), ('greatest', 'JJS'), ('demonstration', 'NN'), ('freedom', 'NN'), ('history', 'NN'), ('nation', 'NN'), ('.', '.')]\n",
            "[('Five', 'CD'), ('score', 'NN'), ('years', 'NNS'), ('ago', 'RB'), (',', ','), ('great', 'JJ'), ('American', 'JJ'), (',', ','), ('whose', 'WP$'), ('symbolic', 'JJ'), ('shadow', 'NN'), ('stand', 'NN'), ('today', 'NN'), (',', ','), ('signed', 'VBN'), ('Emancipation', 'NNP'), ('Proclamation', 'NNP'), ('.', '.')]\n",
            "[('This', 'DT'), ('momentous', 'JJ'), ('decree', 'NN'), ('came', 'VBD'), ('great', 'JJ'), ('beacon', 'NN'), ('light', 'VBD'), ('hope', 'NN'), ('millions', 'NNS'), ('Negro', 'NNP'), ('slaves', 'NNS'), ('.', '.')]\n",
            "[('seared', 'VBN'), ('flames', 'NNS'), ('withering', 'VBG'), ('injustice', 'NN'), ('.', '.')]\n",
            "[('It', 'PRP'), ('came', 'VBD'), ('joyous', 'JJ'), ('daybreak', 'JJ'), ('end', 'NN'), ('long', 'RB'), ('night', 'NN'), ('captivity', 'NN'), ('.', '.')]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VhBUGntnShA4"
      },
      "source": [
        "#Named entity recognition\n",
        "\n",
        "We import the core spaCy English model.\n",
        " Next, we need to create a spaCy document that we will be using to perform parts of speech tagging using nlp().\n",
        " \n",
        "The spaCy document object has several attributes that can be used to perform a variety of tasks. For instance:\n",
        "- to print the text of the document, the text attribute is used. \n",
        "- the pos_ attribute returns the coarse-grained POS tag. \n",
        "- To obtain fine-grained POS tags, we could use the tag_ attribute.\n",
        "- to get the explanation of a tag, we can use the spacy.explain() method and pass it the tag name."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uh7mOjawPXLy"
      },
      "source": [
        "import spacy\n",
        "from spacy import displacy\n",
        "from collections import Counter\n",
        "import en_core_web_sm\n",
        "nlp = en_core_web_sm.load()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z0XoQbEvYnRm",
        "outputId": "001576bb-65ab-492e-9943-57c906faa748"
      },
      "source": [
        "sen = nlp('I like music. I stopped learning , but still like to listen')\n",
        "for word in sen:\n",
        "     print(f'{word.text:{12}} {word.pos_:{10}} {word.tag_:{8}} {spacy.explain(word.tag_)}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "I            PRON       PRP      pronoun, personal\n",
            "like         VERB       VBP      verb, non-3rd person singular present\n",
            "music        NOUN       NN       noun, singular or mass\n",
            ".            PUNCT      .        punctuation mark, sentence closer\n",
            "I            PRON       PRP      pronoun, personal\n",
            "stopped      VERB       VBD      verb, past tense\n",
            "learning     VERB       VBG      verb, gerund or present participle\n",
            ",            PUNCT      ,        punctuation mark, comma\n",
            "but          CCONJ      CC       conjunction, coordinating\n",
            "still        ADV        RB       adverb\n",
            "like         VERB       VBP      verb, non-3rd person singular present\n",
            "to           PART       TO       infinitival \"to\"\n",
            "listen       VERB       VB       verb, base form\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GUO744nNViNo",
        "outputId": "7b6d4b45-1ccd-4035-fa0c-f64c022da243"
      },
      "source": [
        " # NER  \n",
        "sentence = \"Google announces acquisition of Fitbit for $2.1 billion in 2019. Wearables brand Fitbit is now a Google company after the recent $2.1 billion acquisition.\"\n",
        "doc = nlp(sentence) \n",
        "#in output first column is name , next two are span column numbers and last column specifies the category \n",
        "for ent in doc.ents: \n",
        "    print(ent.text, ent.start_char, ent.end_char, ent.label_) \n",
        "\n",
        "print(spacy.explain(\"NORP\"))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Google 0 6 ORG\n",
            "Fitbit 32 38 ORG\n",
            "$2.1 billion 43 55 MONEY\n",
            "2019 59 63 DATE\n",
            "Wearables 65 74 NORP\n",
            "Fitbit 81 87 PERSON\n",
            "Google 97 103 ORG\n",
            "$2.1 billion 129 141 MONEY\n",
            "Nationalities or religious or political groups\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mJ5aXvEyXRGY"
      },
      "source": [
        "spaCy supports the following entity types:\n",
        "PERSON, NORP (nationalities, religious and political groups), FAC (buildings, airports etc.), ORG (organizations), GPE (countries, cities etc.), LOC (mountain ranges, water bodies etc.), PRODUCT (products), EVENT (event names), WORK_OF_ART (books, song titles), LAW (legal document titles), LANGUAGE (named languages), DATE, TIME, PERCENT, MONEY, QUANTITY, ORDINAL and CARDINAL."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lb8XuT7PdZYY"
      },
      "source": [
        "word embeddings - next"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vIZgn5VUdcvL"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}